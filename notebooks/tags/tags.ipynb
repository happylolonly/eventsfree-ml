{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "is_production = os.environ.get('ENV') == 'production'\n",
    "home = os.environ.get('HOME')\n",
    "\n",
    "print('is production', is_production)\n",
    "\n",
    "vectors_path = vectors_path = '/tmp/' if is_production else home + '/ml-data/fasttext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(vectors_path + 'cc.ru.300.bin'):\n",
    "    !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.ru.300.bin.gz -P {vectors_path}\n",
    "    !gunzip {vectors_path + 'cc.ru.300.bin.gz'}\n",
    "        \n",
    "if not os.path.isfile(vectors_path + 'cc.ru.300.vec'):\n",
    "    !wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.ru.300.vec.gz -P {vectors_path}\n",
    "    !gunzip {vectors_path + 'cc.ru.300.vec.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_production:\n",
    "    import pymongo\n",
    "    import pandas as pd\n",
    "    from pymongo import MongoClient\n",
    "\n",
    "    mongo_uri = os.environ['MONGO_URI']\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client.cubes\n",
    "    collection = db.events\n",
    "    data = pd.DataFrame(list(collection.find({ \"$where\": \"this.tags && this.tags.length > 0\" }, { 'title': 1, 'text': 1, 'tags': 1, '_id': 0 })))\n",
    "    data.to_csv('./data.csv')\n",
    "else:\n",
    "    !wget https://raw.githubusercontent.com/happylolonly/eventsfree-ml/master/notebooks/tags/data/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data.csv')\n",
    "df = df.replace(r'\\\\n',' ', regex=True) \n",
    "\n",
    "df['tags'] = df['tags'].map(eval) \n",
    "df['tags'] = df['tags'].map(lambda x: set(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove lines where only 1 uniqie tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for tags in df['tags'].values:\n",
    "    for t in tags:\n",
    "        counter[t] += 1\n",
    "print(counter)\n",
    "\n",
    "arr = []\n",
    "for i in counter:\n",
    "    if counter[i] > 2:\n",
    "        arr.append(i);\n",
    "        \n",
    "print('total count:', len(counter), '\\n >2 tags:', len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_white_list = set(arr)\n",
    "df['tags'] = df['tags'].map(lambda x: set(x).intersection(tags_white_list))\n",
    "df = df[df.tags.map(len) > 0]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "from stop_words import get_stop_words\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "import json\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].apply(strip_tags)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^а-яА-Яa-zA-Z]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\s+\", \" \")\n",
    "    \n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    \n",
    "    stop_words = get_stop_words('russian')\n",
    "    \n",
    "    def stem (text):\n",
    "        temp = []\n",
    "        for word in text.split(\" \"):\n",
    "            if len(word) > 2 and word not in stop_words:\n",
    "                temp.append(stemmer.stem(word))\n",
    "        return \" \".join(temp)\n",
    "        \n",
    "            \n",
    "    df[text_field] = df[text_field].apply(stem)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_text(df, 'text')\n",
    "df = standardize_text(df, 'title')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for row in df.to_dict(orient='record'):\n",
    "    for tag in row['tags']:\n",
    "        row_copy = row.copy()\n",
    "#         print(tag)\n",
    "#         if tag == 'лекция':\n",
    "#             tag = 1\n",
    "#         else:\n",
    "#             tag = 0\n",
    "        row_copy['tag'] = tag\n",
    "        \n",
    "        rows.append(row_copy)\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df['fulltext'] = df[['text', 'title']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# print(df.shape)\n",
    "# print(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3, stratify=df['tag'].values)\n",
    "\n",
    "print(np.unique(train['tag'].values, return_counts=True))\n",
    "print(np.unique(test['tag'].values, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install -U -q git+https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "import fastText\n",
    "\n",
    "FIELD = 'fulltext'\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df, path):\n",
    "    with open(path, 'w+') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write('__label__{} {}\\n'.format(row['tag'], row[FIELD]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(df, '/tmp/train.data')\n",
    "save(train, '/tmp/train.data')\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not is_production:\n",
    "    print('Start train not production')\n",
    "    model = fastText.train_supervised('/tmp/train.data',\n",
    "                                  pretrainedVectors=vectors_path + 'cc.ru.300.vec',\n",
    "                                  dim=300,\n",
    "                                  thread=15,\n",
    "                                  epoch=50,\n",
    "                                  verbose=8)\n",
    "    \n",
    "    model.quantize(qnorm=True, cutoff=100000)\n",
    "    model.save_model('../../server/ml/tags/model/tags_model_new')\n",
    "    print('Model updated')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels, probs = model.predict(test[FIELD].tolist(), k=5)\n",
    "# labels = [ll[0].replace('__label__', '') for ll in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(test['tag'].values, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if is_production:\n",
    "    print('Start train production')\n",
    "    save(df, '/tmp/train.data')\n",
    "    model = fastText.train_supervised('/tmp/train.data',\n",
    "#                                   pretrainedVectors=vectors_path + 'cc.ru.300.vec',\n",
    "                                  dim=300,\n",
    "                                  thread=15,\n",
    "                                  epoch=50,\n",
    "                                  verbose=8)\n",
    "\n",
    "\n",
    "    model.quantize(qnorm=True, cutoff=100000)\n",
    "    model.save_model('../../server/ml/tags/model/tags_model_new')\n",
    "    print('Production model updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, probs = model.predict(test[FIELD].tolist(), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = 0;\n",
    "for i, item in enumerate(test['tags']):\n",
    "    length = len(item)\n",
    "    \n",
    "    prediction = map(lambda x: x.replace('__label__', ''), labels[i][0:length:])\n",
    "    prediction = set(prediction)\n",
    "    \n",
    "    if (item != prediction):\n",
    "        print(item, prediction)\n",
    "        mistakes += 1\n",
    "        \n",
    "print('Accuracy:', (test.shape[0] - mistakes) / test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = 0;\n",
    "for i, item in enumerate(test['tags']):\n",
    "    length = len(item)\n",
    "    \n",
    "    prediction = map(lambda x: x.replace('__label__', ''), labels[i][0:1:])\n",
    "#     print(labels)\n",
    "#     print(prediction)\n",
    "    \n",
    "#     print(prediction in item)\n",
    "    \n",
    "    is_mis = False\n",
    "    for value in prediction:\n",
    "        if (value not in item):\n",
    "            is_mis = True\n",
    "#     prediction = set(prediction)\n",
    "\n",
    "    if (is_mis):\n",
    "        mistakes += 1\n",
    "#         print(prediction)\n",
    "    \n",
    "#     if (item != prediction):\n",
    "#         print(item, prediction)\n",
    "#         mistakes += 1\n",
    "        \n",
    "print('Accuracy:', (test.shape[0] - mistakes) / test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
