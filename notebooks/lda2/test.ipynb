{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../lda/data/text.events.csv', error_bad_lines=False);\n",
    "data_text = data[['text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31 августа @ Бар \"Bristle\", 19:30\\nSDL\\n• Вход...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Лето в Минске начинается с августа!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Появился новый мурал от Рамона Мартинса. Как в...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The topic for the next debate club meeting is ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMPORTANT NEWS!!!\\n\\nThe tomorrow 's meeting i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  index\n",
       "0  31 августа @ Бар \"Bristle\", 19:30\\nSDL\\n• Вход...      0\n",
       "1                Лето в Минске начинается с августа!      1\n",
       "2  Появился новый мурал от Рамона Мартинса. Как в...      2\n",
       "3  The topic for the next debate club meeting is ...      3\n",
       "4  IMPORTANT NEWS!!!\\n\\nThe tomorrow 's meeting i...      4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/happylol/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пойти\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>flies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>dies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mules</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>denied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>owned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>sized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>stating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>itemization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensationa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>traditiona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>referenc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colonizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plotted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word      stemmed\n",
       "0       caresses     caresses\n",
       "1          flies        flies\n",
       "2           dies         dies\n",
       "3          mules        mules\n",
       "4         denied       denied\n",
       "5           died         died\n",
       "6         agreed       agreed\n",
       "7          owned        owned\n",
       "8        humbled      humbled\n",
       "9          sized        sized\n",
       "10       meeting      meeting\n",
       "11       stating      stating\n",
       "12       siezing      siezing\n",
       "13   itemization  itemization\n",
       "14   sensational   sensationa\n",
       "15   traditional   traditiona\n",
       "16     reference     referenc\n",
       "17     colonizer    colonizer\n",
       "18       plotted      plotted"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def preprocess(text):\n",
    "    if (type(text) == float):\n",
    "        print(text);\n",
    "        text = 'тест';\n",
    "    text = strip_tags(text);\n",
    "    \n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['\\\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '<p>18', '&#x43C;&#x430;&#x44F;', '&#x432;', '17.00', '&#x414;&#x43C;&#x438;&#x442;&#x440;&#x438;&#x439;', '&#x41F;&#x43E;&#x431;&#x435;&#x434;&#x438;&#x43D;&#x441;&#x43A;&#x438;&#x439;', '&#x432;', '&#x441;&#x43A;&#x432;&#x435;&#x440;&#x435;', '&#x43F;&#x435;&#x440;&#x435;&#x434;', '&#x433;&#x43B;&#x430;&#x432;&#x43D;&#x44B;&#x43C;', '&#x43A;&#x43E;&#x440;&#x43F;&#x443;&#x441;&#x43E;&#x43C;', '&#x411;&#x41D;&#x422;&#x423;', '&#x43D;&#x430;', '&#x41C;&#x43E;&#x43B;&#x43E;&#x434;&#x435;&#x436;&#x43D;&#x43E;&#x439;', '&#x43F;&#x43B;&#x43E;&#x449;&#x430;&#x434;&#x43A;&#x435;', '&#x43F;&#x440;&#x43E;&#x447;&#x438;&#x442;&#x430;&#x435;&#x442;', '&#x43B;&#x435;&#x43A;&#x446;&#x438;&#x44E;', '&#xAB;&#x421;&#x43A;&#x430;&#x437;&#x43A;&#x430;', '&#x43B;&#x43E;&#x436;&#x44C;,', '&#x434;&#x430;', '&#x432;', '&#x43D;&#x435;&#x439;', '&#x43D;&#x430;&#x43C;&#x435;&#x43A;&#xBB;.</p>\\\\n<p>&#x427;&#x438;&#x442;&#x430;&#x44F;', '&#x444;&#x430;&#x43D;&#x442;&#x430;&#x441;&#x442;&#x438;&#x43A;&#x443;,', '&#x43F;&#x440;&#x43E;&#x441;&#x43C;&#x430;&#x442;&#x440;&#x438;&#x432;&#x430;&#x44F;', '&#x444;&#x438;&#x43B;&#x44C;&#x43C;&#x44B;,', '&#x43C;&#x44B;', '&#x447;&#x430;&#x441;&#x442;&#x43E;', '&#x43C;&#x435;&#x447;&#x442;&#x430;&#x435;&#x43C;,', '&#x447;&#x442;&#x43E;&#x431;&#x44B;', '&#x443;&#x434;&#x438;&#x432;&#x438;&#x442;&#x435;&#x43B;&#x44C;&#x43D;&#x44B;&#x435;', '&#x432;&#x435;&#x449;&#x438;,', '&#x43A;&#x43E;&#x442;&#x43E;&#x440;&#x44B;&#x435;', '&#x442;&#x430;&#x43C;', '&#x43F;&#x440;&#x43E;&#x438;&#x441;&#x445;&#x43E;&#x434;&#x44F;&#x442;,', '&#x43F;&#x440;&#x43E;&#x438;&#x441;&#x445;&#x43E;&#x434;&#x438;&#x43B;&#x438;', '&#x43D;&#x430;&#x44F;&#x432;&#x443;!', '&#x421;&#x442;&#x430;&#x442;&#x44C;', '&#x43D;&#x435;&#x432;&#x438;&#x434;&#x438;&#x43C;&#x43A;&#x43E;&#x439;,', '&#x43F;&#x43E;&#x43C;&#x430;&#x445;&#x430;&#x442;&#x44C;', '&#x43B;&#x430;&#x437;&#x435;&#x440;&#x43D;&#x44B;&#x43C;', '&#x43C;&#x435;&#x447;&#x43E;&#x43C;,', '&#x447;&#x438;&#x442;&#x430;&#x442;&#x44C;', '&#x43C;&#x44B;&#x441;&#x43B;&#x438;', '&#x43B;&#x44E;&#x434;&#x435;&#x439;,', '&#x43F;&#x443;&#x442;&#x435;&#x448;&#x435;&#x441;&#x442;&#x432;&#x43E;&#x432;&#x430;&#x442;&#x44C;', '&#x432;&#x43E;', '&#x432;&#x440;&#x435;&#x43C;&#x435;&#x43D;&#x438;', '&#x438;', '&#x43C;&#x43D;&#x43E;&#x433;&#x43E;&#x435;', '&#x434;&#x440;&#x443;&#x433;&#x43E;&#x435;!', '&#x41E;&#x43A;&#x430;&#x437;&#x44B;&#x432;&#x430;&#x435;&#x442;&#x441;&#x44F;,', '&#x43C;&#x43D;&#x43E;&#x433;&#x43E;&#x435;', '&#x438;&#x437;', '&#x44D;&#x442;&#x43E;&#x433;&#x43E;', '&#x43C;&#x43E;&#x436;&#x43D;&#x43E;', '&#x441;&#x434;&#x435;&#x43B;&#x430;&#x442;&#x44C;,', '&#x445;&#x43E;&#x440;&#x43E;&#x448;&#x43E;', '&#x440;&#x430;&#x437;&#x431;&#x438;&#x440;&#x430;&#x44F;&#x441;&#x44C;', '&#x432;', '&#x444;&#x438;&#x437;&#x438;&#x43A;&#x435;!', '&#x422;&#x430;&#x43A;', '&#x447;&#x442;&#x43E;,', '&#x441;&#x434;&#x435;&#x43B;&#x430;&#x442;&#x44C;', '&#x441;&#x43A;&#x430;&#x437;&#x43A;&#x443;', '&#x431;&#x44B;&#x43B;&#x44C;&#x44E;', '-', '&#x43D;&#x430;&#x43C;&#x43D;&#x43E;&#x433;&#x43E;', '&#x43F;&#x440;&#x43E;&#x449;&#x435;,', '&#x447;&#x435;&#x43C;', '&#x43A;&#x430;&#x436;&#x435;&#x442;&#x441;&#x44F;!&#xA0;</p>\\\\n<p>&#x410;', '&#x432;', '18.00', '&#x43D;&#x430;', '&#x442;&#x43E;&#x439;', '&#x436;&#x435;', '&#x43F;&#x43B;&#x43E;&#x449;&#x430;&#x434;&#x43A;&#x435;', '&#x43D;&#x430;&#x447;&#x43D;&#x435;&#x442;&#x441;&#x44F;', '&#x43B;&#x435;&#x43A;&#x446;&#x438;&#x44F;', '&#x201C;&#x41D;&#x430;&#x441;&#x43A;&#x43E;&#x43B;&#x44C;&#x43A;&#x43E;', '&#x43E;&#x433;&#x440;&#x43E;&#x43C;&#x43D;&#x430;', '&#x412;&#x441;&#x435;&#x43B;&#x435;&#x43D;&#x43D;&#x430;&#x44F;&#xBB;', '&#x41D;&#x43E;&#x447;&#x44C;&#x44E;,', '&#x43D;&#x430;', '&#x43D;&#x435;&#x431;&#x43E;&#x441;&#x432;&#x43E;&#x434;&#x435;', '&#x43D;&#x430;&#x434;', '&#x43D;&#x430;&#x43C;&#x438;', '&#x440;&#x430;&#x441;&#x441;&#x44B;&#x43F;&#x430;&#x43D;&#x44B;', '&#x43C;&#x438;&#x43B;&#x43B;&#x438;&#x43E;&#x43D;&#x44B;', '&#x437;&#x432;&#x435;&#x437;&#x434;,', '&#x442;&#x443;&#x43C;&#x430;&#x43D;&#x43D;&#x43E;&#x441;&#x442;&#x435;&#x439;,', '&#x433;&#x430;&#x43B;&#x430;&#x43A;&#x442;&#x438;&#x43A;!', '&#x41E;&#x43D;&#x438;', '&#x43A;&#x430;&#x436;&#x443;&#x442;&#x441;&#x44F;', '&#x442;&#x430;&#x43A;&#x438;&#x43C;&#x438;', '&#x445;&#x43E;&#x43B;&#x43E;&#x434;&#x43D;&#x44B;&#x43C;&#x438;', '&#x438;', '&#x434;&#x430;&#x43B;&#x435;&#x43A;&#x438;&#x43C;&#x438;.', '&#x41D;&#x43E;,', '&#x435;&#x441;&#x43B;&#x438;', '&#x431;&#x44B;&#x442;&#x44C;', '&#x442;&#x43E;&#x447;&#x43D;&#x44B;&#x43C;,', '&#x43A;&#x430;&#x43A;&#x43E;&#x435;', '&#x434;&#x43E;', '&#x43D;&#x438;&#x445;', '&#x440;&#x430;&#x441;&#x441;&#x442;&#x43E;&#x44F;&#x43D;&#x438;&#x435;?', '&#x41D;&#x430;&#x441;&#x43A;&#x43E;&#x43B;&#x44C;&#x43A;&#x43E;', '&#x434;&#x430;&#x43B;&#x435;&#x43A;&#x43E;', '&#x438;&#x43B;&#x438;', '&#x431;&#x43B;&#x438;&#x437;&#x43A;&#x43E;', '&#x43E;&#x43D;&#x438;', '&#x432;&#x441;&#x435;', '&#x43D;&#x430;&#x445;&#x43E;&#x434;&#x44F;&#x442;&#x441;&#x44F;?', '&#x421;&#x43A;&#x43E;&#x43B;&#x44C;&#x43A;&#x43E;', '&#x432;&#x440;&#x435;&#x43C;&#x435;&#x43D;&#x438;', '&#x43B;&#x435;&#x442;&#x435;&#x442;&#x44C;', '&#x434;&#x43E;', '&#x41B;&#x443;&#x43D;&#x44B;,', '&#x434;&#x440;&#x443;&#x433;&#x438;&#x445;', '&#x43F;&#x43B;&#x430;&#x43D;&#x435;&#x442;,', '&#x431;&#x43B;&#x438;&#x436;&#x430;&#x439;&#x448;&#x438;&#x445;', '&#x437;&#x432;&#x435;&#x437;&#x434;', '&#x438;', '&#x433;&#x430;&#x43B;&#x430;&#x43A;&#x442;&#x438;&#x43A;?', '&#x41D;&#x430;', '&#x43B;&#x435;&#x43A;&#x446;&#x438;&#x438;', '&#x43C;&#x44B;', '&#x441;&#x43E;&#x432;&#x435;&#x440;&#x448;&#x438;&#x43C;', '&#x43C;&#x430;&#x43B;&#x435;&#x43D;&#x44C;&#x43A;&#x43E;&#x435;', '&#x43A;&#x43E;&#x441;&#x43C;&#x438;&#x447;&#x435;&#x441;&#x43A;&#x43E;&#x435;', '&#x43F;&#x443;&#x442;&#x435;&#x448;&#x435;&#x441;&#x442;&#x432;&#x438;&#x435;', '&#x447;&#x443;&#x442;&#x44C;', '&#x43B;&#x438;', '&#x43D;&#x435;', '&#x434;&#x43E;', '&#x43A;&#x440;&#x430;&#x44F;', '&#x412;&#x441;&#x435;&#x43B;&#x435;&#x43D;&#x43D;&#x43E;&#x439;,', '&#x435;&#x441;&#x43B;&#x438;', '&#x43E;&#x43D;', '&#x435;&#x441;&#x442;&#x44C;,', '&#x43A;&#x43E;&#x43D;&#x435;&#x447;&#x43D;&#x43E;', '&#x436;&#x435;.&#xA0;</p>\\\\n<p><strong>&#x41E;', '&#x43F;&#x440;&#x435;&#x43C;&#x438;&#x438;:</strong><br>&#x41F;&#x440;&#x435;&#x43C;&#x438;&#x44F;', '&#xAB;&#x41F;&#x440;&#x43E;&#x441;&#x432;&#x435;&#x442;&#x438;&#x442;&#x435;&#x43B;&#x44C;&#xBB;', '&#x437;&#x430;', '&#x43B;&#x443;&#x447;&#x448;&#x443;&#x44E;', '&#x43D;&#x430;&#x443;&#x447;&#x43D;&#x43E;-&#x43F;&#x43E;&#x43F;&#x443;&#x43B;&#x44F;&#x440;&#x43D;&#x443;&#x44E;', '&#x43A;&#x43D;&#x438;&#x433;&#x443;', '&#x43D;&#x430;', '&#x440;&#x443;&#x441;&#x441;&#x43A;&#x43E;&#x43C;', '&#x44F;&#x437;&#x44B;&#x43A;&#x435;', '&#x431;&#x44B;&#x43B;&#x430;', '&#x443;&#x447;&#x440;&#x435;&#x436;&#x434;&#x435;&#x43D;&#x430;', '&#x432;', '2008', '&#x433;&#x43E;&#x434;&#x443;', '&#x43E;&#x441;&#x43D;&#x43E;&#x432;&#x430;&#x442;&#x435;&#x43B;&#x435;&#x43C;', '&#x43A;&#x43E;&#x43C;&#x43F;&#x430;&#x43D;&#x438;&#x438;', '&#xAB;&#x412;&#x44B;&#x43C;&#x43F;&#x435;&#x43B;&#x43A;&#x43E;&#x43C;&#xBB;', '&#x414;&#x43C;&#x438;&#x442;&#x440;&#x438;&#x435;&#x43C;', '&#x417;&#x438;&#x43C;&#x438;&#x43D;&#x44B;&#x43C;', '&#x438;', '&#x424;&#x43E;&#x43D;&#x434;&#x43E;&#x43C;', '&#x43D;&#x435;&#x43A;&#x43E;&#x43C;&#x43C;&#x435;&#x440;&#x447;&#x435;&#x441;&#x43A;&#x438;&#x445;', '&#x43F;&#x440;&#x43E;&#x433;&#x440;&#x430;&#x43C;&#x43C;', '&#xAB;&#x414;&#x438;&#x43D;&#x430;&#x441;&#x442;&#x438;&#x44F;&#xBB;.', '&#x421;', '2016', '&#x433;&#x43E;&#x434;&#x430;', '&#x43F;&#x440;&#x435;&#x43C;&#x438;&#x44F;', '&#x432;&#x440;&#x443;&#x447;&#x430;&#x435;&#x442;&#x441;&#x44F;', '&#x43F;&#x440;&#x438;', '&#x43F;&#x43E;&#x434;&#x434;&#x435;&#x440;&#x436;&#x43A;&#x435;', 'ZiminFoundation.', '&#x426;&#x435;&#x43B;&#x44C;', '&#x43F;&#x440;&#x435;&#x43C;&#x438;&#x438;', '&#x2013;', '&#x43F;&#x440;&#x438;&#x432;&#x43B;&#x435;&#x447;&#x44C;', '&#x432;&#x43D;&#x438;&#x43C;&#x430;&#x43D;&#x438;&#x435;', '&#x447;&#x438;&#x442;&#x430;&#x442;&#x435;&#x43B;&#x435;&#x439;', '&#x43A;', '&#x43F;&#x440;&#x43E;&#x441;&#x432;&#x435;&#x442;&#x438;&#x442;&#x435;&#x43B;&#x44C;&#x441;&#x43A;&#x43E;&#x43C;&#x443;', '&#x436;&#x430;&#x43D;&#x440;&#x443;,', '&#x43F;&#x43E;&#x43E;&#x449;&#x440;&#x438;&#x442;&#x44C;', '&#x430;&#x432;&#x442;&#x43E;&#x440;&#x43E;&#x432;', '&#x438;', '&#x441;&#x43E;&#x437;&#x434;&#x430;&#x442;&#x44C;', '&#x43F;&#x440;&#x435;&#x434;&#x43F;&#x43E;&#x441;&#x44B;&#x43B;&#x43A;&#x438;', '&#x434;&#x43B;&#x44F;', '&#x440;&#x430;&#x441;&#x448;&#x438;&#x440;&#x435;&#x43D;&#x438;&#x44F;', '&#x440;&#x44B;&#x43D;&#x43A;&#x430;', '&#x43F;&#x440;&#x43E;&#x441;&#x432;&#x435;&#x442;&#x438;&#x442;&#x435;&#x43B;&#x44C;&#x441;&#x43A;&#x43E;&#x439;', '&#x43B;&#x438;&#x442;&#x435;&#x440;&#x430;&#x442;&#x443;&#x440;&#x44B;', '&#x432;', '&#x420;&#x43E;&#x441;&#x441;&#x438;&#x438;.', '&#x412;', '2017', '&#x433;&#x43E;&#x434;&#x443;', '&#x43F;&#x440;&#x435;&#x43C;&#x438;&#x438;', '&#xAB;&#x41F;&#x440;&#x43E;&#x441;&#x432;&#x435;&#x442;&#x438;&#x442;&#x435;&#x43B;&#x44C;&#xBB;', '&#x438;&#x441;&#x43F;&#x43E;&#x43B;&#x43D;&#x44F;&#x435;&#x442;&#x441;&#x44F;', '10', '&#x43B;&#x435;&#x442;.</p>\\\\n<p>&#x421;&#x43E;&#x43E;&#x440;&#x433;&#x430;&#x43D;&#x438;&#x437;&#x430;&#x442;&#x43E;&#x440;&#x44B;', '&#x432;&#x441;&#x442;&#x440;&#x435;&#x447;&#x438;', '&#x2013;', '&#x43F;&#x440;&#x43E;&#x435;&#x43A;&#x442;', '&#xAB;&#x41D;&#x430;&#x443;&#x447;&#x43A;&#x43E;&#x442;&#xBB;', '&#x438;', '&#x43C;&#x430;&#x433;&#x430;&#x437;&#x438;&#x43D;&#xA0;<span><a', 'href=\"https://media.by/\"', 'target=\"_blank\"><span>media.by</span></a></span></p>\\\\n', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['дмитр', 'побединск', 'сквер', 'перед', 'главн', 'корпус', 'бнту', 'молодежн', 'площадк', 'прочита', 'лекц', 'сказк', 'лож', 'намек', 'нчит', 'фантастик', 'просматрив', 'фильм', 'част', 'мечта', 'чтоб', 'удивительн', 'вещ', 'котор', 'происход', 'происход', 'наяв', 'стат', 'невидимк', 'помаха', 'лазерн', 'меч', 'чита', 'мысл', 'люд', 'путешествова', 'времен', 'мног', 'друг', 'оказыва', 'мног', 'эт', 'можн', 'сдела', 'хорош', 'разбир', 'физик', 'сдела', 'сказк', 'был', 'намн', 'прощ', 'кажет', 'площадк', 'начнет', 'лекц', 'наскольк', 'огромн', 'вселен', 'ноч', 'небосвод', 'нам', 'рассыпа', 'миллион', 'звезд', 'туман', 'галактик', 'кажут', 'так', 'холодн', 'далек', 'есл', 'быт', 'точн', 'как', 'расстоян', 'наскольк', 'далек', 'близк', 'наход', 'скольк', 'времен', 'летет', 'лун', 'друг', 'планет', 'ближайш', 'звезд', 'галактик', 'лекц', 'соверш', 'маленьк', 'космическ', 'путешеств', 'чут', 'кра', 'вселен', 'есл', 'ест', 'конечн', 'прем', 'прем', 'просветител', 'лучш', 'научн', 'популярн', 'книг', 'русск', 'язык', 'был', 'учрежд', 'год', 'основател', 'компан', 'вымпелк', 'дмитр', 'зимин', 'фонд', 'некоммерческ', 'программ', 'династ', 'год', 'прем', 'вруча', 'поддержк', 'ziminfoundation', 'цел', 'прем', 'привлеч', 'вниман', 'читател', 'жанр', 'поощр', 'автор', 'созда', 'предпосылк', 'расширен', 'рынк', 'литератур', 'росс', 'год', 'прем', 'просветител', 'исполня', 'нсоорганизатор', 'встреч', 'проект', 'научкот', 'магазин', 'med']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x1a2bffd438>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "map(lambda x,y,z : print(x), [1, 2, 3, 4]) #Output [2, 4, 6, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "processed_docs = documents['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [август, bristl, nsdl, вход, свободн, нзаказ, ...\n",
       "1                         [лет, минск, начина, август]\n",
       "2                    [появ, нов, мура, рамон, мартинс]\n",
       "3    [topic, debat, club, meet, euthanas, justif, l...\n",
       "4    [important, news, nthe, tomorrow, meet, cancel...\n",
       "5                                               [март]\n",
       "6    [sorr, lat, announcement, meet, hold, toda, us...\n",
       "7         [topic, husband, wiv, cook, dinner, weekend]\n",
       "8       [meet, hold, toda, usua, worr, cancel, librar]\n",
       "9    [подписыва, https, facebook, alfabankb, сообще...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bristl\n",
      "1 nsdl\n",
      "2 август\n",
      "3 вход\n",
      "4 нзаказ\n",
      "5 свободн\n",
      "6 стол\n",
      "7 лет\n",
      "8 минск\n",
      "9 начина\n",
      "10 мартинс\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 1),\n",
       " (59, 1),\n",
       " (60, 2),\n",
       " (72, 2),\n",
       " (81, 1),\n",
       " (82, 2),\n",
       " (83, 1),\n",
       " (100, 1),\n",
       " (106, 1),\n",
       " (110, 1),\n",
       " (115, 1),\n",
       " (126, 1),\n",
       " (137, 1),\n",
       " (139, 1),\n",
       " (146, 1),\n",
       " (148, 2),\n",
       " (167, 3),\n",
       " (179, 1),\n",
       " (183, 1),\n",
       " (207, 1),\n",
       " (209, 1),\n",
       " (215, 1),\n",
       " (226, 1),\n",
       " (235, 1),\n",
       " (237, 1),\n",
       " (243, 1),\n",
       " (280, 1),\n",
       " (325, 1),\n",
       " (339, 1),\n",
       " (488, 1),\n",
       " (499, 1),\n",
       " (511, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (537, 1),\n",
       " (588, 2),\n",
       " (668, 1),\n",
       " (670, 2),\n",
       " (738, 1),\n",
       " (739, 1),\n",
       " (810, 1),\n",
       " (814, 1),\n",
       " (821, 1),\n",
       " (851, 2),\n",
       " (905, 2),\n",
       " (933, 1),\n",
       " (1017, 1),\n",
       " (1037, 1),\n",
       " (1093, 1),\n",
       " (1115, 1),\n",
       " (1127, 1),\n",
       " (1134, 2),\n",
       " (1242, 1),\n",
       " (1248, 1),\n",
       " (1277, 3),\n",
       " (1330, 1),\n",
       " (1345, 1),\n",
       " (1352, 1),\n",
       " (1366, 1),\n",
       " (1398, 1),\n",
       " (1422, 1),\n",
       " (1471, 1),\n",
       " (1596, 1),\n",
       " (1606, 1),\n",
       " (1610, 1),\n",
       " (1613, 1),\n",
       " (1632, 1),\n",
       " (1659, 1),\n",
       " (1826, 1),\n",
       " (1878, 1),\n",
       " (1881, 1),\n",
       " (1895, 1),\n",
       " (1921, 2),\n",
       " (2072, 2),\n",
       " (2111, 1),\n",
       " (2191, 5),\n",
       " (2272, 1),\n",
       " (2448, 2),\n",
       " (2449, 1),\n",
       " (2462, 1),\n",
       " (2527, 2),\n",
       " (2665, 1),\n",
       " (2669, 1),\n",
       " (2767, 1),\n",
       " (2849, 1),\n",
       " (3138, 1),\n",
       " (3174, 1),\n",
       " (3317, 1),\n",
       " (3471, 1),\n",
       " (3514, 1),\n",
       " (4373, 1),\n",
       " (4514, 2),\n",
       " (4624, 1),\n",
       " (5011, 1),\n",
       " (5362, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 44 (\"компан\") appears 1 time.\n",
      "Word 59 (\"ближайш\") appears 1 time.\n",
      "Word 60 (\"был\") appears 2 time.\n",
      "Word 72 (\"звезд\") appears 2 time.\n",
      "Word 81 (\"мечта\") appears 1 time.\n",
      "Word 82 (\"мног\") appears 2 time.\n",
      "Word 83 (\"молодежн\") appears 1 time.\n",
      "Word 100 (\"русск\") appears 1 time.\n",
      "Word 106 (\"стат\") appears 1 time.\n",
      "Word 110 (\"фильм\") appears 1 time.\n",
      "Word 115 (\"чтоб\") appears 1 time.\n",
      "Word 126 (\"встреч\") appears 1 time.\n",
      "Word 137 (\"магазин\") appears 1 time.\n",
      "Word 139 (\"можн\") appears 1 time.\n",
      "Word 146 (\"путешеств\") appears 1 time.\n",
      "Word 148 (\"сдела\") appears 2 time.\n",
      "Word 167 (\"год\") appears 3 time.\n",
      "Word 179 (\"котор\") appears 1 time.\n",
      "Word 183 (\"люд\") appears 1 time.\n",
      "Word 207 (\"планет\") appears 1 time.\n",
      "Word 209 (\"поддержк\") appears 1 time.\n",
      "Word 215 (\"проект\") appears 1 time.\n",
      "Word 226 (\"созда\") appears 1 time.\n",
      "Word 235 (\"цел\") appears 1 time.\n",
      "Word 237 (\"эт\") appears 1 time.\n",
      "Word 243 (\"вниман\") appears 1 time.\n",
      "Word 280 (\"программ\") appears 1 time.\n",
      "Word 325 (\"лучш\") appears 1 time.\n",
      "Word 339 (\"популярн\") appears 1 time.\n",
      "Word 488 (\"исполня\") appears 1 time.\n",
      "Word 499 (\"начнет\") appears 1 time.\n",
      "Word 511 (\"перед\") appears 1 time.\n",
      "Word 527 (\"хорош\") appears 1 time.\n",
      "Word 528 (\"язык\") appears 1 time.\n",
      "Word 533 (\"главн\") appears 1 time.\n",
      "Word 534 (\"ест\") appears 1 time.\n",
      "Word 537 (\"миллион\") appears 1 time.\n",
      "Word 588 (\"есл\") appears 2 time.\n",
      "Word 668 (\"быт\") appears 1 time.\n",
      "Word 670 (\"времен\") appears 2 time.\n",
      "Word 738 (\"ноч\") appears 1 time.\n",
      "Word 739 (\"огромн\") appears 1 time.\n",
      "Word 810 (\"нам\") appears 1 time.\n",
      "Word 814 (\"точн\") appears 1 time.\n",
      "Word 821 (\"конечн\") appears 1 time.\n",
      "Word 851 (\"друг\") appears 2 time.\n",
      "Word 905 (\"происход\") appears 2 time.\n",
      "Word 933 (\"част\") appears 1 time.\n",
      "Word 1017 (\"как\") appears 1 time.\n",
      "Word 1037 (\"скольк\") appears 1 time.\n",
      "Word 1093 (\"автор\") appears 1 time.\n",
      "Word 1115 (\"корпус\") appears 1 time.\n",
      "Word 1127 (\"оказыва\") appears 1 time.\n",
      "Word 1134 (\"площадк\") appears 2 time.\n",
      "Word 1242 (\"основател\") appears 1 time.\n",
      "Word 1248 (\"рынк\") appears 1 time.\n",
      "Word 1277 (\"лекц\") appears 3 time.\n",
      "Word 1330 (\"бнту\") appears 1 time.\n",
      "Word 1345 (\"литератур\") appears 1 time.\n",
      "Word 1352 (\"намн\") appears 1 time.\n",
      "Word 1366 (\"росс\") appears 1 time.\n",
      "Word 1398 (\"книг\") appears 1 time.\n",
      "Word 1422 (\"прощ\") appears 1 time.\n",
      "Word 1471 (\"привлеч\") appears 1 time.\n",
      "Word 1596 (\"прочита\") appears 1 time.\n",
      "Word 1606 (\"так\") appears 1 time.\n",
      "Word 1610 (\"фонд\") appears 1 time.\n",
      "Word 1613 (\"чита\") appears 1 time.\n",
      "Word 1632 (\"наход\") appears 1 time.\n",
      "Word 1659 (\"med\") appears 1 time.\n",
      "Word 1826 (\"маленьк\") appears 1 time.\n",
      "Word 1878 (\"близк\") appears 1 time.\n",
      "Word 1881 (\"вещ\") appears 1 time.\n",
      "Word 1895 (\"лун\") appears 1 time.\n",
      "Word 1921 (\"сказк\") appears 2 time.\n",
      "Word 2072 (\"дмитр\") appears 2 time.\n",
      "Word 2111 (\"научн\") appears 1 time.\n",
      "Word 2191 (\"прем\") appears 5 time.\n",
      "Word 2272 (\"чут\") appears 1 time.\n",
      "Word 2448 (\"вселен\") appears 2 time.\n",
      "Word 2449 (\"космическ\") appears 1 time.\n",
      "Word 2462 (\"физик\") appears 1 time.\n",
      "Word 2527 (\"далек\") appears 2 time.\n",
      "Word 2665 (\"жанр\") appears 1 time.\n",
      "Word 2669 (\"расстоян\") appears 1 time.\n",
      "Word 2767 (\"расширен\") appears 1 time.\n",
      "Word 2849 (\"мысл\") appears 1 time.\n",
      "Word 3138 (\"удивительн\") appears 1 time.\n",
      "Word 3174 (\"путешествова\") appears 1 time.\n",
      "Word 3317 (\"холодн\") appears 1 time.\n",
      "Word 3471 (\"соверш\") appears 1 time.\n",
      "Word 3514 (\"некоммерческ\") appears 1 time.\n",
      "Word 4373 (\"читател\") appears 1 time.\n",
      "Word 4514 (\"наскольк\") appears 2 time.\n",
      "Word 4624 (\"кажет\") appears 1 time.\n",
      "Word 5011 (\"кра\") appears 1 time.\n",
      "Word 5362 (\"сквер\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3652728867821245),\n",
      " (1, 0.15328224415414574),\n",
      " (2, 0.7701415527749449),\n",
      " (3, 0.1508904738877443),\n",
      " (4, 0.4766490589398566)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"занят\" + 0.009*\"котор\" + 0.008*\"сво\" + 0.007*\"курс\" + 0.007*\"вход\" + 0.007*\"бесплатн\" + 0.007*\"свободн\" + 0.007*\"навык\" + 0.006*\"нов\" + 0.005*\"язык\"\n",
      "Topic: 1 \n",
      "Words: 0.011*\"вход\" + 0.010*\"сторон\" + 0.009*\"мероприят\" + 0.008*\"котор\" + 0.008*\"можн\" + 0.007*\"сво\" + 0.007*\"чтоб\" + 0.007*\"cub\" + 0.007*\"чут\" + 0.007*\"свободн\"\n",
      "Topic: 2 \n",
      "Words: 0.008*\"котор\" + 0.008*\"проект\" + 0.007*\"минск\" + 0.007*\"бесплатн\" + 0.006*\"вход\" + 0.006*\"нов\" + 0.005*\"такж\" + 0.005*\"будет\" + 0.005*\"пройдет\" + 0.005*\"создан\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"котор\" + 0.009*\"сво\" + 0.008*\"компан\" + 0.007*\"минск\" + 0.006*\"будет\" + 0.005*\"можн\" + 0.005*\"как\" + 0.005*\"мероприят\" + 0.004*\"бесплатн\" + 0.004*\"эт\"\n",
      "Topic: 4 \n",
      "Words: 0.018*\"проект\" + 0.009*\"бизнес\" + 0.008*\"управлен\" + 0.007*\"курс\" + 0.006*\"работ\" + 0.006*\"компан\" + 0.005*\"сво\" + 0.004*\"участ\" + 0.004*\"команд\" + 0.004*\"club\"\n",
      "Topic: 5 \n",
      "Words: 0.022*\"дизайн\" + 0.011*\"част\" + 0.010*\"практическ\" + 0.008*\"market\" + 0.008*\"основ\" + 0.008*\"продуктов\" + 0.007*\"приложен\" + 0.007*\"работ\" + 0.007*\"тестирован\" + 0.006*\"сервис\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"drupa\" + 0.005*\"работ\" + 0.005*\"врем\" + 0.005*\"разработк\" + 0.005*\"session\" + 0.005*\"frontend\" + 0.005*\"эт\" + 0.005*\"lead\" + 0.005*\"котор\" + 0.005*\"инструмент\"\n",
      "Topic: 7 \n",
      "Words: 0.010*\"минск\" + 0.008*\"котор\" + 0.007*\"проект\" + 0.007*\"нов\" + 0.006*\"билет\" + 0.006*\"розыгрыш\" + 0.006*\"будет\" + 0.006*\"бизнес\" + 0.005*\"групп\" + 0.005*\"сво\"\n",
      "Topic: 8 \n",
      "Words: 0.011*\"https\" + 0.008*\"wall\" + 0.007*\"класс\" + 0.006*\"мастер\" + 0.006*\"регистрац\" + 0.006*\"блокчейн\" + 0.005*\"компан\" + 0.005*\"бесплатн\" + 0.005*\"участник\" + 0.005*\"бизнес\"\n",
      "Topic: 9 \n",
      "Words: 0.011*\"сво\" + 0.009*\"минск\" + 0.007*\"наш\" + 0.007*\"котор\" + 0.006*\"свободн\" + 0.006*\"работ\" + 0.006*\"вопрос\" + 0.005*\"будет\" + 0.005*\"выставк\" + 0.005*\"опыт\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"проект\" + 0.003*\"компан\" + 0.003*\"бизнес\" + 0.002*\"сво\" + 0.002*\"ноябр\" + 0.002*\"сам\" + 0.002*\"котор\" + 0.002*\"приз\" + 0.002*\"минск\" + 0.002*\"регистрац\"\n",
      "Topic: 1 Word: 0.003*\"тренировк\" + 0.003*\"криптовалют\" + 0.003*\"фитнес\" + 0.002*\"занят\" + 0.002*\"нача\" + 0.002*\"club\" + 0.002*\"ноябр\" + 0.002*\"бесплатн\" + 0.002*\"минск\" + 0.002*\"developer\"\n",
      "Topic: 2 Word: 0.004*\"дизайн\" + 0.002*\"занят\" + 0.002*\"курс\" + 0.002*\"групп\" + 0.002*\"танц\" + 0.002*\"класс\" + 0.002*\"продуктов\" + 0.002*\"част\" + 0.002*\"вход\" + 0.002*\"бесплатн\"\n",
      "Topic: 3 Word: 0.007*\"вход\" + 0.006*\"свободн\" + 0.005*\"wall\" + 0.003*\"https\" + 0.003*\"групп\" + 0.003*\"занят\" + 0.003*\"концерт\" + 0.002*\"ноябр\" + 0.002*\"проект\" + 0.002*\"билет\"\n",
      "Topic: 4 Word: 0.004*\"билет\" + 0.004*\"drupa\" + 0.003*\"frontend\" + 0.003*\"минск\" + 0.002*\"розыгрыш\" + 0.002*\"colour\" + 0.002*\"club\" + 0.002*\"котор\" + 0.002*\"свободн\" + 0.002*\"вход\"\n",
      "Topic: 5 Word: 0.010*\"вход\" + 0.010*\"свободн\" + 0.005*\"club\" + 0.003*\"розыгрыш\" + 0.003*\"репост\" + 0.003*\"сво\" + 0.003*\"долж\" + 0.003*\"https\" + 0.003*\"определ\" + 0.002*\"комик\"\n",
      "Topic: 6 Word: 0.003*\"вход\" + 0.003*\"класс\" + 0.003*\"дизайн\" + 0.003*\"свободн\" + 0.003*\"танц\" + 0.003*\"мастер\" + 0.003*\"занят\" + 0.002*\"минск\" + 0.002*\"декабр\" + 0.002*\"бесплатн\"\n",
      "Topic: 7 Word: 0.004*\"занят\" + 0.003*\"язык\" + 0.003*\"студент\" + 0.003*\"пауз\" + 0.003*\"коф\" + 0.003*\"будут\" + 0.003*\"встреч\" + 0.002*\"английск\" + 0.002*\"нов\" + 0.002*\"котор\"\n",
      "Topic: 8 Word: 0.008*\"arcor\" + 0.008*\"android\" + 0.006*\"создан\" + 0.006*\"constrainlayout\" + 0.006*\"ark\" + 0.006*\"увидел\" + 0.005*\"отлич\" + 0.005*\"кирилл\" + 0.005*\"appl\" + 0.005*\"заполн\"\n",
      "Topic: 9 Word: 0.004*\"seavus\" + 0.004*\"свободн\" + 0.004*\"вход\" + 0.003*\"опыт\" + 0.003*\"наш\" + 0.003*\"codetalks\" + 0.003*\"brest\" + 0.003*\"встреч\" + 0.003*\"специалист\" + 0.003*\"сво\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['дмитр',\n",
       " 'побединск',\n",
       " 'сквер',\n",
       " 'перед',\n",
       " 'главн',\n",
       " 'корпус',\n",
       " 'бнту',\n",
       " 'молодежн',\n",
       " 'площадк',\n",
       " 'прочита',\n",
       " 'лекц',\n",
       " 'сказк',\n",
       " 'лож',\n",
       " 'намек',\n",
       " 'нчит',\n",
       " 'фантастик',\n",
       " 'просматрив',\n",
       " 'фильм',\n",
       " 'част',\n",
       " 'мечта',\n",
       " 'чтоб',\n",
       " 'удивительн',\n",
       " 'вещ',\n",
       " 'котор',\n",
       " 'происход',\n",
       " 'происход',\n",
       " 'наяв',\n",
       " 'стат',\n",
       " 'невидимк',\n",
       " 'помаха',\n",
       " 'лазерн',\n",
       " 'меч',\n",
       " 'чита',\n",
       " 'мысл',\n",
       " 'люд',\n",
       " 'путешествова',\n",
       " 'времен',\n",
       " 'мног',\n",
       " 'друг',\n",
       " 'оказыва',\n",
       " 'мног',\n",
       " 'эт',\n",
       " 'можн',\n",
       " 'сдела',\n",
       " 'хорош',\n",
       " 'разбир',\n",
       " 'физик',\n",
       " 'сдела',\n",
       " 'сказк',\n",
       " 'был',\n",
       " 'намн',\n",
       " 'прощ',\n",
       " 'кажет',\n",
       " 'площадк',\n",
       " 'начнет',\n",
       " 'лекц',\n",
       " 'наскольк',\n",
       " 'огромн',\n",
       " 'вселен',\n",
       " 'ноч',\n",
       " 'небосвод',\n",
       " 'нам',\n",
       " 'рассыпа',\n",
       " 'миллион',\n",
       " 'звезд',\n",
       " 'туман',\n",
       " 'галактик',\n",
       " 'кажут',\n",
       " 'так',\n",
       " 'холодн',\n",
       " 'далек',\n",
       " 'есл',\n",
       " 'быт',\n",
       " 'точн',\n",
       " 'как',\n",
       " 'расстоян',\n",
       " 'наскольк',\n",
       " 'далек',\n",
       " 'близк',\n",
       " 'наход',\n",
       " 'скольк',\n",
       " 'времен',\n",
       " 'летет',\n",
       " 'лун',\n",
       " 'друг',\n",
       " 'планет',\n",
       " 'ближайш',\n",
       " 'звезд',\n",
       " 'галактик',\n",
       " 'лекц',\n",
       " 'соверш',\n",
       " 'маленьк',\n",
       " 'космическ',\n",
       " 'путешеств',\n",
       " 'чут',\n",
       " 'кра',\n",
       " 'вселен',\n",
       " 'есл',\n",
       " 'ест',\n",
       " 'конечн',\n",
       " 'прем',\n",
       " 'прем',\n",
       " 'просветител',\n",
       " 'лучш',\n",
       " 'научн',\n",
       " 'популярн',\n",
       " 'книг',\n",
       " 'русск',\n",
       " 'язык',\n",
       " 'был',\n",
       " 'учрежд',\n",
       " 'год',\n",
       " 'основател',\n",
       " 'компан',\n",
       " 'вымпелк',\n",
       " 'дмитр',\n",
       " 'зимин',\n",
       " 'фонд',\n",
       " 'некоммерческ',\n",
       " 'программ',\n",
       " 'династ',\n",
       " 'год',\n",
       " 'прем',\n",
       " 'вруча',\n",
       " 'поддержк',\n",
       " 'ziminfoundation',\n",
       " 'цел',\n",
       " 'прем',\n",
       " 'привлеч',\n",
       " 'вниман',\n",
       " 'читател',\n",
       " 'жанр',\n",
       " 'поощр',\n",
       " 'автор',\n",
       " 'созда',\n",
       " 'предпосылк',\n",
       " 'расширен',\n",
       " 'рынк',\n",
       " 'литератур',\n",
       " 'росс',\n",
       " 'год',\n",
       " 'прем',\n",
       " 'просветител',\n",
       " 'исполня',\n",
       " 'нсоорганизатор',\n",
       " 'встреч',\n",
       " 'проект',\n",
       " 'научкот',\n",
       " 'магазин',\n",
       " 'med']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9324227571487427\t \n",
      "Topic: 0.011*\"котор\" + 0.009*\"сво\" + 0.008*\"компан\" + 0.007*\"минск\" + 0.006*\"будет\" + 0.005*\"можн\" + 0.005*\"как\" + 0.005*\"мероприят\" + 0.004*\"бесплатн\" + 0.004*\"эт\"\n",
      "\n",
      "Score: 0.06090875342488289\t \n",
      "Topic: 0.010*\"минск\" + 0.008*\"котор\" + 0.007*\"проект\" + 0.007*\"нов\" + 0.006*\"билет\" + 0.006*\"розыгрыш\" + 0.006*\"будет\" + 0.006*\"бизнес\" + 0.005*\"групп\" + 0.005*\"сво\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.47126305103302\t \n",
      "Topic: 0.003*\"тренировк\" + 0.003*\"криптовалют\" + 0.003*\"фитнес\" + 0.002*\"занят\" + 0.002*\"нача\" + 0.002*\"club\" + 0.002*\"ноябр\" + 0.002*\"бесплатн\" + 0.002*\"минск\" + 0.002*\"developer\"\n",
      "\n",
      "Score: 0.41878241300582886\t \n",
      "Topic: 0.004*\"seavus\" + 0.004*\"свободн\" + 0.004*\"вход\" + 0.003*\"опыт\" + 0.003*\"наш\" + 0.003*\"codetalks\" + 0.003*\"brest\" + 0.003*\"встреч\" + 0.003*\"специалист\" + 0.003*\"сво\"\n",
      "\n",
      "Score: 0.10411956161260605\t \n",
      "Topic: 0.004*\"занят\" + 0.003*\"язык\" + 0.003*\"студент\" + 0.003*\"пауз\" + 0.003*\"коф\" + 0.003*\"будут\" + 0.003*\"встреч\" + 0.002*\"английск\" + 0.002*\"нов\" + 0.002*\"котор\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5499588251113892\t Topic: [('дизайн', 0.022040477), ('част', 0.01067833), ('практическ', 0.009945214), ('market', 0.008382885), ('основ', 0.008192588)]\n",
      "Score: 0.05001766234636307\t Topic: [('https', 0.011197289), ('wall', 0.0075064427), ('класс', 0.006875984), ('мастер', 0.0060046487), ('регистрац', 0.0058637937)]\n",
      "Score: 0.05000755935907364\t Topic: [('котор', 0.010753264), ('сво', 0.009428455), ('компан', 0.008309926), ('минск', 0.0072592236), ('будет', 0.00648109)]\n",
      "Score: 0.05000488832592964\t Topic: [('минск', 0.009603944), ('котор', 0.007834163), ('проект', 0.006955929), ('нов', 0.0069356035), ('билет', 0.0064230976)]\n",
      "Score: 0.050003860145807266\t Topic: [('проект', 0.018113881), ('бизнес', 0.008508109), ('управлен', 0.008262795), ('курс', 0.0067854123), ('работ', 0.00601669)]\n",
      "Score: 0.0500032976269722\t Topic: [('котор', 0.008194484), ('проект', 0.007674819), ('минск', 0.0073017804), ('бесплатн', 0.0066998606), ('вход', 0.00559491)]\n",
      "Score: 0.05000164732336998\t Topic: [('вход', 0.010568807), ('сторон', 0.010151291), ('мероприят', 0.009377429), ('котор', 0.0080573615), ('можн', 0.0076861456)]\n",
      "Score: 0.05000076815485954\t Topic: [('занят', 0.010884883), ('котор', 0.009211702), ('сво', 0.008355862), ('курс', 0.0074884016), ('вход', 0.007030627)]\n",
      "Score: 0.05000076815485954\t Topic: [('сво', 0.011270078), ('минск', 0.008935191), ('наш', 0.007155884), ('котор', 0.006753731), ('свободн', 0.0060651707)]\n",
      "Score: 0.05000073090195656\t Topic: [('drupa', 0.0069025736), ('работ', 0.0054414826), ('врем', 0.005376374), ('разработк', 0.0052755643), ('session', 0.005166797)]\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.show_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamulticore.LdaMulticore at 0x1a31f5a4a8>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from gensim.corpora import Dictionary\n",
    "# tmp_fname = 'l23321';\n",
    "# dictionary.save_as_text(tmp_fname)\n",
    "# loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
    "# loaded_dct\n",
    "lda = lda_model\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    " # Save model to disk.\n",
    "temp_file = datapath(\"./model\")\n",
    "lda.save('model')\n",
    "# Load a potentially pretrained model from disk.\n",
    "\n",
    "lda = gensim.models.LdaMulticore.load('model')\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
